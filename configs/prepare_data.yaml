data_path: '../input/'
test_size: 0.1
random_state: 42

dataframes:
    path: '../input/dataframes/'

tokenizer:
    fit: True
    path: '../input/tokenizer'
    model: 'WordPiece'
    file: 'tokenizer'
    vocab_size: 100

images:
    resize: True
    size: 224
    dilate:
        kernel: 2
        iterations: 2
